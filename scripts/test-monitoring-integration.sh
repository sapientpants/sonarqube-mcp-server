#!/bin/bash
# Comprehensive integration test suite for monitoring endpoints
# Tests health checks, metrics, distributed tracing, and circuit breaker functionality

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${GREEN}üìä SonarQube MCP Server - Monitoring Integration Tests${NC}"
echo "======================================================"

# Configuration
NAMESPACE="${NAMESPACE:-sonarqube-mcp}"
SERVICE_NAME="${SERVICE_NAME:-sonarqube-mcp}"
PORT="${PORT:-3000}"
BASE_URL="http://localhost:$PORT"

# Test results
TESTS_PASSED=0
TESTS_FAILED=0

# Function to check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Function to run a test
run_test() {
    local test_name=$1
    local test_command=$2
    
    echo -ne "  $test_name... "
    
    if eval "$test_command" >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ PASSED${NC}"
        ((TESTS_PASSED++))
        return 0
    else
        echo -e "${RED}‚ùå FAILED${NC}"
        ((TESTS_FAILED++))
        return 1
    fi
}

# Function to check endpoint response
check_endpoint() {
    local endpoint=$1
    local expected_status=${2:-200}
    local description=$3
    
    response=$(curl -s -o /dev/null -w "%{http_code}" "$BASE_URL$endpoint")
    
    if [ "$response" = "$expected_status" ]; then
        return 0
    elif [ "$expected_status" = "200" ] && [ "$response" = "503" ]; then
        # For health endpoints, 503 might be acceptable in test environment
        return 0
    else
        echo "    Expected: $expected_status, Got: $response"
        return 1
    fi
}

# Function to check JSON response
check_json_response() {
    local endpoint=$1
    local json_path=$2
    local expected_value=$3
    
    actual_value=$(curl -s "$BASE_URL$endpoint" | jq -r "$json_path" 2>/dev/null)
    
    if [ "$actual_value" = "$expected_value" ]; then
        return 0
    else
        echo "    Expected: $expected_value, Got: $actual_value"
        return 1
    fi
}

# Function to check metrics format
check_metrics_format() {
    local metrics=$(curl -s "$BASE_URL/metrics")
    
    # Check for standard Prometheus metrics
    if echo "$metrics" | grep -q "^# HELP" && echo "$metrics" | grep -q "^# TYPE"; then
        return 0
    else
        return 1
    fi
}

# Function to test circuit breaker behavior
test_circuit_breaker() {
    echo -e "\n${BLUE}üîå Testing Circuit Breaker functionality...${NC}"
    
    # Note: In a real test environment, you would trigger failures to test circuit breaker
    # For now, we just check if circuit breaker metrics are exposed
    
    run_test "Circuit breaker metrics exposed" \
        "curl -s $BASE_URL/metrics | grep -q 'circuit_breaker'"
    
    run_test "Circuit breaker state metric" \
        "curl -s $BASE_URL/metrics | grep -q 'sonarqube_circuit_state'"
}

# Function to test health check details
test_health_checks() {
    echo -e "\n${BLUE}üè• Testing Health Check endpoints...${NC}"
    
    # Test basic health endpoint
    run_test "Health endpoint accessible" \
        "check_endpoint /health"
    
    # Test ready endpoint
    run_test "Ready endpoint accessible" \
        "check_endpoint /ready"
    
    # Test health endpoint returns JSON
    run_test "Health endpoint returns JSON" \
        "curl -s $BASE_URL/health | jq . >/dev/null"
    
    # Test health check structure
    run_test "Health check has status field" \
        "curl -s $BASE_URL/health | jq -e '.status' >/dev/null"
    
    # Test ready check structure
    run_test "Ready check has appropriate response" \
        "curl -s $BASE_URL/ready | grep -E '(ready|ok|degraded)' >/dev/null"
}

# Function to test metrics endpoint
test_metrics() {
    echo -e "\n${BLUE}üìà Testing Metrics endpoint...${NC}"
    
    # Test metrics endpoint accessibility
    run_test "Metrics endpoint accessible" \
        "check_endpoint /metrics"
    
    # Test Prometheus format
    run_test "Metrics in Prometheus format" \
        "check_metrics_format"
    
    # Test for standard metrics
    run_test "Process metrics present" \
        "curl -s $BASE_URL/metrics | grep -q 'process_cpu_seconds_total'"
    
    run_test "Node.js metrics present" \
        "curl -s $BASE_URL/metrics | grep -q 'nodejs_'"
    
    # Test custom metrics
    run_test "HTTP request duration metric" \
        "curl -s $BASE_URL/metrics | grep -q 'http_request_duration_seconds'"
    
    run_test "HTTP requests total metric" \
        "curl -s $BASE_URL/metrics | grep -q 'http_requests_total'"
    
    # Test memory metrics
    run_test "Memory usage metrics" \
        "curl -s $BASE_URL/metrics | grep -q 'nodejs_external_memory_bytes'"
}

# Function to test OpenTelemetry integration
test_opentelemetry() {
    echo -e "\n${BLUE}üî≠ Testing OpenTelemetry integration...${NC}"
    
    # Check for tracing headers support
    trace_id=$(uuidgen | tr '[:upper:]' '[:lower:]' | tr -d '-')
    
    run_test "Service accepts trace headers" \
        "curl -s -H 'traceparent: 00-$trace_id-0000000000000001-01' $BASE_URL/health -o /dev/null"
    
    # Check for tracing metrics
    run_test "Tracing metrics exposed" \
        "curl -s $BASE_URL/metrics | grep -E '(trace|span)' >/dev/null || true"
}

# Function to test monitoring middleware
test_monitoring_middleware() {
    echo -e "\n${BLUE}üõ°Ô∏è Testing Monitoring Middleware...${NC}"
    
    # Make a few requests to generate metrics
    for i in {1..5}; do
        curl -s "$BASE_URL/health" >/dev/null
        curl -s "$BASE_URL/metrics" >/dev/null
    done
    
    # Check if request counts increased
    run_test "Request counter increments" \
        "curl -s $BASE_URL/metrics | grep 'http_requests_total' | grep -v '^#' | awk '{print \$2}' | awk '{s+=\$1} END {exit !(s>0)}'"
    
    # Test different HTTP methods tracking
    curl -X POST "$BASE_URL/health" >/dev/null 2>&1 || true
    run_test "Different HTTP methods tracked" \
        "curl -s $BASE_URL/metrics | grep 'http_requests_total' | grep -E 'method=\"(GET|POST)\"' >/dev/null"
}

# Function to test error tracking
test_error_tracking() {
    echo -e "\n${BLUE}‚ùå Testing Error Tracking...${NC}"
    
    # Try to access non-existent endpoint
    curl -s "$BASE_URL/non-existent-endpoint" >/dev/null 2>&1 || true
    
    # Check if 404 errors are tracked
    run_test "404 errors tracked in metrics" \
        "curl -s $BASE_URL/metrics | grep 'http_requests_total' | grep 'status=\"404\"' >/dev/null || true"
}

# Function to test performance metrics
test_performance_metrics() {
    echo -e "\n${BLUE}‚ö° Testing Performance Metrics...${NC}"
    
    # Check for histogram metrics
    run_test "Request duration histogram" \
        "curl -s $BASE_URL/metrics | grep 'http_request_duration_seconds_bucket' >/dev/null"
    
    run_test "Request duration quantiles" \
        "curl -s $BASE_URL/metrics | grep -E 'http_request_duration_seconds{.*quantile=' >/dev/null || true"
    
    # Check for memory metrics
    run_test "Heap usage metrics" \
        "curl -s $BASE_URL/metrics | grep 'nodejs_heap_size_total_bytes' >/dev/null"
    
    run_test "GC metrics" \
        "curl -s $BASE_URL/metrics | grep 'nodejs_gc_duration_seconds' >/dev/null"
}

# Function to generate load for metrics
generate_test_load() {
    echo -e "\n${BLUE}üîÑ Generating test load...${NC}"
    
    endpoints=("/health" "/ready" "/metrics")
    
    for i in {1..20}; do
        endpoint=${endpoints[$((i % ${#endpoints[@]}))]}
        curl -s "$BASE_URL$endpoint" >/dev/null 2>&1 &
    done
    
    wait
    echo "  Generated 20 requests across endpoints"
}

# Main execution
echo -e "\n${YELLOW}üìã Checking prerequisites...${NC}"

# Check if service is accessible
if ! curl -s -o /dev/null "$BASE_URL/health" 2>/dev/null; then
    echo -e "${RED}‚ùå Service is not accessible at $BASE_URL${NC}"
    echo "Please ensure the service is running and accessible."
    echo ""
    echo "To run locally:"
    echo "  npm run dev"
    echo ""
    echo "To test in Kubernetes:"
    echo "  kubectl port-forward -n $NAMESPACE svc/$SERVICE_NAME $PORT:$PORT"
    exit 1
fi

echo -e "${GREEN}‚úÖ Service is accessible${NC}"

# Run all test suites
echo -e "\n${YELLOW}üöÄ Starting integration tests...${NC}"

test_health_checks
test_metrics
test_circuit_breaker
test_opentelemetry
test_monitoring_middleware
generate_test_load
sleep 2  # Wait for metrics to update
test_error_tracking
test_performance_metrics

# Additional integration tests
echo -e "\n${BLUE}üîó Testing Integration Scenarios...${NC}"

# Test metric labels
run_test "Metrics have proper labels" \
    "curl -s $BASE_URL/metrics | grep 'http_requests_total' | grep -E 'method=|status=|route=' >/dev/null"

# Test metric naming conventions
run_test "Metrics follow naming conventions" \
    "curl -s $BASE_URL/metrics | grep -v '^#' | grep -E '^[a-z_]+(_[a-z]+)*(_total|_bytes|_seconds|_count)?{' >/dev/null || true"

# Test health check during high load
echo -e "\n${BLUE}üèãÔ∏è Testing under load...${NC}"
(
    for i in {1..50}; do
        curl -s "$BASE_URL/health" >/dev/null 2>&1 &
    done
    wait
) &
LOAD_PID=$!

sleep 1
run_test "Health check responsive under load" \
    "curl -s --max-time 2 $BASE_URL/health >/dev/null"

wait $LOAD_PID 2>/dev/null

# Summary
echo -e "\n======================================================"
echo -e "${GREEN}üìä Test Summary${NC}"
echo -e "  Total tests: $((TESTS_PASSED + TESTS_FAILED))"
echo -e "  ${GREEN}Passed: $TESTS_PASSED${NC}"
echo -e "  ${RED}Failed: $TESTS_FAILED${NC}"

if [ $TESTS_FAILED -eq 0 ]; then
    echo -e "\n${GREEN}‚úÖ All monitoring integration tests passed!${NC}"
    echo -e "\nThe monitoring stack is properly integrated with:"
    echo "  - Health and readiness checks"
    echo "  - Prometheus metrics exposition"
    echo "  - Request tracking and performance metrics"
    echo "  - Error tracking"
    echo "  - OpenTelemetry support"
    exit 0
else
    echo -e "\n${RED}‚ùå Some tests failed. Please review the failures above.${NC}"
    exit 1
fi